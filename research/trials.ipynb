{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m venv genai\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35283527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd \"C:\\Users\\Lenovo\\Desktop\\Gen AI\"\n",
    "# genai\\Scripts\\activate\n",
    "# pip install -r requirement.txt\n",
    "\n",
    "\n",
    "# git rm --cached .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e739063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# If using ConversationalRetrievalChain (legacy):\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb2217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting clear, concise, and effective instructions to guide AI models towards generating desired outputs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Option 1: set the env variable before running your script\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key\"\n",
    "\n",
    "# Option 2: pass the API key explicitly\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", google_api_key=api_key)\n",
    "\n",
    "response = llm.invoke(\"Explain prompt engineering in one sentence.\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4dba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi     ,there how are you']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "get_text_chunks(\" Hi     ,there how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70683f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bd9a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example chunks of text\n",
    "text_chunks = [\n",
    "    \"First chunk of text to embed.\",\n",
    "    \"Second chunk of text, maybe a paragraph.\",\n",
    "    \"More text chunks can be added here.\",\n",
    "    \"Boiler Tubes are very rare material that withstand lot of pressure\",\n",
    "    \"Which is better? Watertube or Firetube boiler?\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(text_chunks)\n",
    "\n",
    "# # Print the embeddings\n",
    "# for i, embedding in enumerate(embeddings):\n",
    "#     print(f\"Embedding {i}:\", embedding)\n",
    "#     print(f\"Vector dimension: {len(embedding)}\")\n",
    "\n",
    "# Save embeddings and texts locally as .npy and .txt files\n",
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "with open('text_chunks.txt', 'w', encoding='utf-8') as f:\n",
    "    for chunk in text_chunks:\n",
    "        f.write(chunk.replace('\\n', ' ') + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30286a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching chunk: First chunk of text to embed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load embeddings and texts\n",
    "embeddings = np.load('embeddings.npy')\n",
    "\n",
    "with open('text_chunks.txt', 'r', encoding='utf-8') as f:\n",
    "    text_chunks = [line.strip() for line in f]\n",
    "\n",
    "# Example query\n",
    "query = \"What can you tell me about the embeddings\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Calculate cosine similarities and find best match\n",
    "similarities = cosine_similarity(query_embedding, embeddings)\n",
    "best_match_idx = similarities.argmax()\n",
    "\n",
    "print(\"Best matching chunk:\", text_chunks[best_match_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f04b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d41bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load model & generate embeddings for text_chunks (as before)\n",
    "# embeddings = model.encode(text_chunks)\n",
    "\n",
    "# Convert to float32 for FAISS\n",
    "embedding_vectors = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embedding_vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_vectors)\n",
    "\n",
    "# Save index to file for future use\n",
    "faiss.write_index(index, 'faiss_index.bin')\n",
    "\n",
    "# To save text_chunks for lookup\n",
    "with open('text_chunks.txt', 'w', encoding='utf-8') as f:\n",
    "    for chunk in text_chunks:\n",
    "        f.write(chunk.replace('\\n', ' ') + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb272d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching chunk: Which is better? Watertube or Firetube boiler?, Distance: 0.8428683280944824\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load FAISS index and text chunks\n",
    "index = faiss.read_index('faiss_index.bin')\n",
    "\n",
    "with open('text_chunks.txt', 'r', encoding='utf-8') as f:\n",
    "    text_chunks = [line.strip() for line in f]\n",
    "\n",
    "# Encode query\n",
    "query = \"What can you tell me about the boiler?\"\n",
    "query_vector = model.encode([query]).astype('float32')\n",
    "\n",
    "# Search top 1 nearest neighbor\n",
    "distances, indices = index.search(query_vector, k=1)\n",
    "\n",
    "best_idx = indices[0][0]\n",
    "print(f\"Best matching chunk: {text_chunks[best_idx]}, Distance: {distances[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a476762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Based on the information provided (which is currently empty, but I\\'ll assume general knowledge about boiler tube leakages), here are the different types of boiler tube leakages:\\n\\nBoiler tube leakages can be broadly categorized by the **cause of failure** and the **location of the failure**.\\n\\n**1. Leakages by Cause of Failure:**\\n\\n*   **Erosion:** This occurs when abrasive particles in the flue gas (like ash, coke, or soot) repeatedly impact the tube surface. Over time, this wears away the tube material, thinning it and eventually leading to a hole.\\n    *   **Common locations:** Areas with high gas velocity, such as at the back pass, economizer, or superheater sections.\\n*   **Corrosion:** This is the degradation of the tube material due to chemical reactions.\\n    *   **Water-side Corrosion:** This can be caused by:\\n        *   **Oxygen Pitting:** Dissolved oxygen in boiler water can create localized pits that deepen and eventually perforate the tube.\\n        *   **Caustic Gouging/Embrittlement:** High concentrations of caustic soda (NaOH) can attack steel, leading to thinning and eventual failure. This is often found at stress concentration points.\\n        *   **Acid Attack:** If the boiler water becomes acidic, it can corrode the tube material.\\n    *   **Fire-side Corrosion:** This can be caused by:\\n        *   **Sulfidation:** Sulfur compounds in the fuel ash can react with the tube metal at high temperatures, forming brittle sulfides. This is common in furnaces burning fuels with high sulfur content.\\n        *   **Oxidation:** High-temperature exposure to oxygen can lead to the formation of scale on the tube surface. While some scale is protective, excessive or uneven scaling can lead to overheating and eventual rupture.\\n*   **Overheating/Tube Rupture:** This is a common and often catastrophic failure. It occurs when the tube metal temperature exceeds its design limit, causing it to lose strength and deform under pressure.\\n    *   **Causes of Overheating:**\\n        *   **Scale Buildup (Water-side):** Scale acts as an insulator, preventing efficient heat transfer from the water to the tube metal. This causes the tube metal to get hotter.\\n        *   **Flame Impingement:** Direct contact of the flame with the tubes can locally overheat them.\\n        *   **Low Water Level:** If the tubes are not adequately covered by water, they will overheat.\\n        *   **Circulation Failure:** Inadequate water circulation within the boiler can lead to localized overheating.\\n        *   **Excessive Heat Input:** Exceeding the boiler\\'s design heat input can lead to general overheating.\\n*   **Creep:** At high temperatures and under sustained stress, boiler tubes can slowly deform over time. This deformation can lead to thinning and eventual rupture. Creep is a long-term effect and is more prevalent in high-pressure and high-temperature boilers.\\n*   **Thermal Fatigue:** Repeated cycles of heating and cooling can cause stress on the tube material, leading to the formation and propagation of cracks.\\n*   **Mechanical Damage:** This can include:\\n    *   **Tube Stays/Supports Failure:** If tube supports fail, tubes can sag and lead to stress concentrations.\\n    *   **External Impact:** Accidental impact from tools or equipment during maintenance.\\n    *   **Improper Installation/Repair:** Bending tubes too sharply, poor welding, or using incorrect materials can lead to premature failure.\\n*   **Manufacturing Defects:** Imperfections in the tube material from the manufacturing process, such as inclusions or laminations, can act as initiation sites for failure.\\n\\n**2. Leakages by Location of Failure:**\\n\\n*   **Tube Sheet Leaks:** While not strictly a tube failure, leaks can originate from the joints where tubes are attached to the tube sheet. This can be due to:\\n    *   **Defective Tube-to-Tube Sheet Welds:** Cracks or porosity in the welds.\\n    *   **Corrosion at the Joint:** Galvanic corrosion or crevice corrosion.\\n    *   **Mechanical Stress:** Differential expansion and contraction.\\n*   **Straight Tube Leaks:** Failures occurring in the straight sections of boiler tubes. This can be due to any of the causes mentioned above.\\n*   **Bent Tube Leaks:** Failures occurring in the curved sections of boiler tubes. These areas can be more prone to stress concentrations and thermal fatigue.\\n*   **Header/Manifold Leaks:** While not a tube itself, leaks can occur at the connections to headers or manifolds, often due to gasket failures or corrosion.\\n\\n**Common Manifestations of Leakage:**\\n\\nRegardless of the cause, boiler tube leakages often manifest as:\\n\\n*   **Visible Steam or Water Leaks:** The most obvious sign.\\n*   **\"Whistling\" or \"Hissing\" Sounds:** Due to high-pressure steam escaping.\\n*   **Water Droplets or Wet Spots:** On the boiler casing or surrounding areas.\\n*   **Deterioration of Refractory or Insulation:** If steam is escaping behind these materials.\\n*   **Increased Makeup Water Demand:** Indicating loss of boiler water.\\n*   **Lowered Boiler Pressure:** Due to continuous steam loss.\\n*   **Carryover:** In severe cases, water or steam leaks can affect the steam quality.\\n\\nUnderstanding these different types of leakages is crucial for effective diagnosis, repair, and preventative maintenance of boiler systems.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--08bacf84-a689-4771-8a96-2ce4bc565b62-0' usage_metadata={'input_tokens': 46, 'output_tokens': 1157, 'total_tokens': 1203, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# If using ConversationalRetrievalChain (legacy):\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", google_api_key = GOOGLE_API_KEY)\n",
    "\n",
    "template = '''\n",
    "        You are an expert in answering question about the boiler tube leakage.\n",
    "        here are some relevant information: {relevant_Information}\n",
    "        Here is the question to answer: {question}\n",
    "    '''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "result = chain.invoke({\"relevant_Information\": [], \"question\" : \"What are different types of boiler tube leakages?\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac30c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05558df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modern way of doing things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e524c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting effective instructions and queries for AI models to elicit desired responses.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Option 1: set the env variable before running your script\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key\"\n",
    "\n",
    "# Option 2: pass the API key explicitly\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", google_api_key=api_key)\n",
    "\n",
    "response = llm.invoke(\"Explain prompt engineering in one sentence.\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9314443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant called Universe.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
    "])\n",
    "\n",
    "pipeline = prompt_template | llm\n",
    "\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        # if session ID doesn't exist, create a new chat history\n",
    "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[session_id]\n",
    "\n",
    "\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"query\",\n",
    "    history_messages_key=\"history\"\n",
    ")\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"Hi, my name is Pavan\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")\n",
    "\n",
    "pipeline_with_history.invoke(\n",
    "    {\"query\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84d0ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
