{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67d21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m venv genai\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35283527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd \"C:\\Users\\Lenovo\\Desktop\\Gen AI\"\n",
    "# genai\\Scripts\\activate\n",
    "# pip install -r requirement.txt\n",
    "\n",
    "\n",
    "# git rm --cached .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e739063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "# If using ConversationalRetrievalChain (legacy):\n",
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_classic.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb2217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt engineering is the art and science of crafting clear, concise, and effective instructions to guide AI models towards generating desired outputs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Option 1: set the env variable before running your script\n",
    "# os.environ[\"GOOGLE_API_KEY\"] = \"your_gemini_api_key\"\n",
    "\n",
    "# Option 2: pass the API key explicitly\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", google_api_key=api_key)\n",
    "\n",
    "response = llm.invoke(\"Explain prompt engineering in one sentence.\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e4dba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi     ,there how are you']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 20)\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "get_text_chunks(\" Hi     ,there how are you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70683f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7bd9a3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example chunks of text\n",
    "text_chunks = [\n",
    "    \"First chunk of text to embed.\",\n",
    "    \"Second chunk of text, maybe a paragraph.\",\n",
    "    \"More text chunks can be added here.\",\n",
    "    \"Boiler Tubes are very rare material that withstand lot of pressure\",\n",
    "    \"Which is better? Watertube or Firetube boiler?\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(text_chunks)\n",
    "\n",
    "# # Print the embeddings\n",
    "# for i, embedding in enumerate(embeddings):\n",
    "#     print(f\"Embedding {i}:\", embedding)\n",
    "#     print(f\"Vector dimension: {len(embedding)}\")\n",
    "\n",
    "# Save embeddings and texts locally as .npy and .txt files\n",
    "np.save('embeddings.npy', embeddings)\n",
    "\n",
    "with open('text_chunks.txt', 'w', encoding='utf-8') as f:\n",
    "    for chunk in text_chunks:\n",
    "        f.write(chunk.replace('\\n', ' ') + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30286a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching chunk: First chunk of text to embed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load embeddings and texts\n",
    "embeddings = np.load('embeddings.npy')\n",
    "\n",
    "with open('text_chunks.txt', 'r', encoding='utf-8') as f:\n",
    "    text_chunks = [line.strip() for line in f]\n",
    "\n",
    "# Example query\n",
    "query = \"What can you tell me about the embeddings\"\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Calculate cosine similarities and find best match\n",
    "similarities = cosine_similarity(query_embedding, embeddings)\n",
    "best_match_idx = similarities.argmax()\n",
    "\n",
    "print(\"Best matching chunk:\", text_chunks[best_match_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f04b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d41bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load model & generate embeddings for text_chunks (as before)\n",
    "# embeddings = model.encode(text_chunks)\n",
    "\n",
    "# Convert to float32 for FAISS\n",
    "embedding_vectors = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Create FAISS index\n",
    "dimension = embedding_vectors.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embedding_vectors)\n",
    "\n",
    "# Save index to file for future use\n",
    "faiss.write_index(index, 'faiss_index.bin')\n",
    "\n",
    "# To save text_chunks for lookup\n",
    "with open('text_chunks.txt', 'w', encoding='utf-8') as f:\n",
    "    for chunk in text_chunks:\n",
    "        f.write(chunk.replace('\\n', ' ') + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb272d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best matching chunk: Which is better? Watertube or Firetube boiler?, Distance: 0.8428683280944824\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Load FAISS index and text chunks\n",
    "index = faiss.read_index('faiss_index.bin')\n",
    "\n",
    "with open('text_chunks.txt', 'r', encoding='utf-8') as f:\n",
    "    text_chunks = [line.strip() for line in f]\n",
    "\n",
    "# Encode query\n",
    "query = \"What can you tell me about the boiler?\"\n",
    "query_vector = model.encode([query]).astype('float32')\n",
    "\n",
    "# Search top 1 nearest neighbor\n",
    "distances, indices = index.search(query_vector, k=1)\n",
    "\n",
    "best_idx = indices[0][0]\n",
    "print(f\"Best matching chunk: {text_chunks[best_idx]}, Distance: {distances[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a476762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac30c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05558df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
